{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "based on\n",
        "https://github.com/midimusicgeneration"
      ],
      "metadata": {
        "id": "g9AyruIVbjeQ"
      },
      "id": "g9AyruIVbjeQ"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "246b5c30-b2c8-433f-a34e-276404c03489",
      "metadata": {
        "id": "246b5c30-b2c8-433f-a34e-276404c03489"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "\n",
        "from os import walk\n",
        "import random\n",
        "from torch.utils import data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fd52864e-5d1c-4b85-8fb9-caeb51440b9a",
      "metadata": {
        "id": "fd52864e-5d1c-4b85-8fb9-caeb51440b9a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Music_Data_Utils\n",
        "functions and classes:\n",
        "MidiDataset = Torch data loader. Transforms a list of midi files (midi_files) into a pytorch dataloader\n",
        "list_midi_files = Given a source folder and the number of Train, Validation and test cases that you want, it will return a list of midis for each category\n",
        "'''\n",
        "\n",
        "class MidiDataset(data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, qnsf, seq_len=25, cod_type=2, midi_files=[]):\n",
        "        'Initialization'\n",
        "        if cod_type !=1 and cod_type !=2:\n",
        "          raise TypeError(\"cod_type is not 1 (88 notes) or 2 (176 notes)\")\n",
        "        self.notes = self.get_notes(qnsf=qnsf, cod_type=cod_type, midi_files=midi_files)\n",
        "        self.qnsf = qnsf\n",
        "        self.seq_len = seq_len\n",
        "        self.cod_type = cod_type\n",
        "        self.midi_source = midi_files\n",
        "\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.notes) - self.seq_len*2\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        samples = self.notes[index:(index+self.seq_len*2)]\n",
        "        x=np.asarray(samples).astype(np.float32)\n",
        "\n",
        "        return (x[0:self.seq_len,:],x[self.seq_len:,:])\n",
        "        #return  (samples[0:self.seq_len], samples[self.seq_len:])\n",
        "\n",
        "  def get_notes(self, qnsf, cod_type,midi_files):\n",
        "      \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
        "      notes = []\n",
        "\n",
        "      for index, file in enumerate(midi_files):\n",
        "          try:\n",
        "              print(f\"Processing file {file}\")  # Print file name\n",
        "              midi = converter.parse(file)\n",
        "          except Exception as e:\n",
        "              print(f\"Error processing file {file} (index {index + 1}): {e}\")  # Print error message\n",
        "              continue  # Skip this file and continue with the next one\n",
        "\n",
        "          try: # file has instrument parts\n",
        "              s2 = instrument.partitionByInstrument(midi)\n",
        "              notes_to_parse = s2.parts[0].recurse()\n",
        "          except: # file has notes in a flat structure\n",
        "              notes_to_parse = midi.flat.notes\n",
        "\n",
        "              #initialize piano roll\n",
        "          length=int(notes_to_parse[-1].offset*qnsf)+1 #count number subdivisions\n",
        "          notes_song=np.zeros((length,88*cod_type))\n",
        "\n",
        "          for element in notes_to_parse:\n",
        "              #print(element.offset, element.duration.quarterLength, element.pitch.midi-21)\n",
        "              if isinstance(element, note.Note):\n",
        "                  # cod_type is based on (0,1), (1,0), (0,0)\n",
        "                  notes_song[int(element.offset*qnsf),cod_type*(element.pitch.midi-21)]=1.0\n",
        "                  notes_song[(int(element.offset*qnsf)+1):(int(element.offset*qnsf)+int(element.duration.quarterLength*qnsf)),cod_type*(element.pitch.midi-21)+1]=1.0\n",
        "\n",
        "              elif isinstance(element, chord.Chord):\n",
        "                  for note_in_chord in element.pitches:\n",
        "                    # cod_type is based on (0,1), (1,0), (0,0)\n",
        "                    notes_song[int(element.offset*qnsf),cod_type*(note_in_chord.midi-21)]=1.0\n",
        "                    notes_song[(int(element.offset*qnsf)+1):(int(element.offset*qnsf)+int(element.duration.quarterLength*qnsf)),cod_type*(note_in_chord.midi-21)+1]=1.0\n",
        "              #print(notes_song.shape)\n",
        "\n",
        "          notes+=[list(i) for i in list(notes_song)]\n",
        "      return notes\n",
        "\n",
        "\n",
        "def list_midi_files(midi_source, data_len_train, data_len_val , data_len_test, randomSeq=True, seed = 666):\n",
        "    \"\"\" Given a directory with midi files, return a tuple of 3 lists with the filenames for the train, validation and test sets\n",
        "        The funcion can apply some randomeness in the files selection order although it can be reproduced cause of the seed number\n",
        "    \"\"\"\n",
        "    midi_files_all = []\n",
        "    # iterate over all filenames in the midi_Source\n",
        "    for (dirpath, dirnames, filenames) in walk(midi_source):\n",
        "        midi_files_all.extend(filenames)\n",
        "    midi_files_all = [ glob.glob(midi_source+\"/\"+fi)[0] for fi in midi_files_all if fi.endswith(\".mid\") ]\n",
        "\n",
        "    # we apply som randomnes in the midi selection to prevent some systematic dependences between train, validation and test.\n",
        "    if randomSeq:\n",
        "        random.seed( seed )\n",
        "        midi_files_sel = random.sample(midi_files_all, (data_len_train + data_len_val + data_len_test))\n",
        "    else:\n",
        "        midi_files_sel = midi_files_all\n",
        "    return midi_files_sel[:data_len_train], midi_files_sel[data_len_train:(data_len_train + data_len_val)], midi_files_sel[(data_len_train + data_len_val):]\n",
        "\n",
        "def cleanSeq(x, cod_type):\n",
        "    \"\"\" Given a pianoroll x tith 3 dimensions: batches, timesteps and notes*cod_type, the function returns the\n",
        "        corrected pianoroll adapted to the codificacion rules: a one in the odd position (continuation note),\n",
        "        has to be precedded in the previous timestep by a one in the even or odd position. If exists, this issue,\n",
        "        then we moove this one to the even position.\n",
        "        Another rule is that it can't exist a one in the even and odd position in the same time step. If exists,\n",
        "        then the one in the ood is removed.\n",
        "    \"\"\"\n",
        "    if cod_type==2:\n",
        "        # first row correction\n",
        "        notePos = np.where(x[0,:]==1)[0]        # where are the ones\n",
        "        notePosWrong = np.array([aa%2 for aa in notePos])\n",
        "        notePosWrongPos = np.where(notePosWrong==1)[0]     # we look if corespond to odd positions\n",
        "        x[0,notePos[notePosWrongPos]]=0                    # if present, we correct to 0 the odd\n",
        "        x[0,notePos[notePosWrongPos]-1]=1                  # if present, we correct to 0 the correspondent even position\n",
        "\n",
        "        # other rows correction\n",
        "        # when we have 1 in even and odd position, then remove the second one\n",
        "        x[:,[ii for ii in range(x.shape[1]) if ii%2==1 ] ] = x[:,[ii for ii in range(x.shape[1]) if ii%2==1 ] ] * (1-x)[:,[ii for ii in range(x.shape[1]) if ii%2==0 ] ]\n",
        "\n",
        "        # now we want to compare with previous row\n",
        "        x_0 = x[1:,1:]     #   base values to correct\n",
        "        x_1 = x[0:-1,1:]   #   previous same column\n",
        "        x_11 = x[0:-1,0:-1]#   previous even column\n",
        "        x_sel = np.zeros(x_0.shape)  # odd columns f x\n",
        "        x_sel[:, [ii for ii in range(x_sel.shape[1]) if ii%2==0 ] ] =1   # corespond to even columns of x_sel\n",
        "        x_2 = ((x_1==0) & (x_11==0) & (x_0==1) & (x_sel==1))   # columns with one in odd that not have any in even or odd correspondece in previous row\n",
        "        x[1:,1:][x_2] = 0    # correction in unpair row\n",
        "        x[1:,:-1][x_2] = 1\n",
        "    return(x)\n",
        "\n",
        "\n",
        "def create_midi(prediction_output, qnsf = 4, cod_type=2, midiOutputFile='test_output.mid'):\n",
        "    \"\"\" convert the output (as a numpy array) from the prediction to notes and create a midi file\n",
        "        from the notes\n",
        "    \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    prediction_output=cleanSeq(prediction_output, cod_type=2)\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for i,pattern in enumerate(prediction_output):\n",
        "        # pattern is a note\n",
        "        notes = []\n",
        "        noteIndex = np.where(pattern==1)[0]\n",
        "        noteIndex = noteIndex[[ii % cod_type ==0 for ii in noteIndex]]\n",
        "        if len(noteIndex)>0:\n",
        "            for current_note in noteIndex:\n",
        "                new_note = note.Note(int(current_note/cod_type + 21))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                new_note.offset = offset\n",
        "                # duration.quarterLength in case cod_type==2\n",
        "                if cod_type==2 :\n",
        "                    # sequence of duration equal to one in the odd position\n",
        "                    auxDuration = np.where(prediction_output[(i+1):,current_note + 1]==1)[0]\n",
        "                    # initialize duration\n",
        "                    minimum_value = 0\n",
        "                    if len(auxDuration)>0:\n",
        "                        # minimum position where we have a consecuive sequence at 0\n",
        "                        # we add one to include complete sequences\n",
        "                        minimum_value = np.array(range(len(auxDuration)+1))[~np.isin(range(len(auxDuration)+1),auxDuration)].min()\n",
        "                    # we calcuate the minimum number in the sequance :len(auxDuration) that is not\n",
        "                    # in the sequence, add one and divide by QNSF\n",
        "                    new_note.duration.quarterLength = ( minimum_value + 1.0)/qnsf\n",
        "\n",
        "                output_notes.append(new_note)\n",
        "\n",
        "        offset += 1.0/qnsf\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp=midiOutputFile)\n",
        "    print(\"created midi file: \",midiOutputFile )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "05fbca04-accf-489d-83dc-fa9282da70fd",
      "metadata": {
        "id": "05fbca04-accf-489d-83dc-fa9282da70fd"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Seq2Seq\n",
        "'''\n",
        "\n",
        "# device definition that depends on the gpu availability\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\t# device definition that depends on the gpu availability\n",
        "\t\tdef __init__(self, input_dim, rnn_dim=512, rnn_layers=2, thr=0):\n",
        "\t# input_dim is the pianoroll size\n",
        "\t\t\t\tsuper(Seq2Seq, self).__init__()\n",
        "\n",
        "\t\t\t\tself.thr=thr\n",
        "\t\t\t\tself.input_dim=input_dim\n",
        "\t\t\t\tself.rnn_dim=rnn_dim\n",
        "\t\t\t\tself.rnn_layers=rnn_layers\n",
        "\n",
        "\t\t\t\t# encoder decoder definiction based on LSTM with dropout network\n",
        "\t\t\t\tself.encoder = nn.LSTM( input_size=input_dim, hidden_size=rnn_dim, num_layers=rnn_layers, batch_first=True, dropout=0.2)\n",
        "\t\t\t\tself.decoder = nn.LSTM( input_size=input_dim, hidden_size=rnn_dim, num_layers=rnn_layers, batch_first=True, dropout=0.2)\n",
        "\t\t\t\t# the clasification is based on two fully conected networks dropout\n",
        "\t\t\t\tself.classifier = nn.Sequential(\n",
        "\t\t\t\t\t\tnn.Linear(rnn_dim, 256),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Dropout(0.5),\n",
        "\t\t\t\t\t\tnn.Linear(256, input_dim)\n",
        "\t\t\t\t)\n",
        "\n",
        "\t\t\t\t# loss function\n",
        "\t\t\t\tself.loss_function = nn.BCEWithLogitsLoss() #combines logsoftmax with NLLLoss\n",
        "\n",
        "\t\tdef forward(self, x,y,teacher_forcing_ratio = 0.5):\n",
        "\t\t\t\t# Train method with teacher forcing ratio to introduce more or less real notes in the training step\n",
        "\n",
        "\t\t\t\t# zero padding in the x for the encoder.\n",
        "\t\t\t\tx0 = torch.zeros(x.shape[0], 1, x.shape[2]).to(device)\n",
        "\t\t\t\tx = torch.cat([x0,x], dim=1)\n",
        "\n",
        "\t\t\t\toutput, (hn, cn) = self.encoder(x)\n",
        "\n",
        "\t\t\t\tseq_len = y.shape[1]\n",
        "\n",
        "\t\t\t\t# initializing the output to zeros.\n",
        "\t\t\t\toutputs = torch.zeros(y.shape[0], seq_len, self.input_dim).to(device)\n",
        "\n",
        "\t\t\t\t# zero padding in the y for the decoder\n",
        "\t\t\t\ty0 = torch.zeros(y.shape[0], 1, y.shape[2]).to(device)\n",
        "\t\t\t\ty = torch.cat([y0,y], dim=1)\n",
        "\n",
        "\t\t\t\t# initializing the output to zeros.\n",
        "\t\t\t\tinput = y[:,0,:].view(y.shape[0],1,y.shape[2])\n",
        "\n",
        "\t\t\t\t# we generate iteratively the future time steps selecting th real o predicted note based on a random scheme\n",
        "\t\t\t\t# conditioned to teacher_forcing_ratio value: 1 is 100% teacher forcing, 0 only predicted notes\n",
        "\t\t\t\tfor t in range(1, seq_len):\n",
        "\t\t\t\t\t\toutput, (hn, cn) = self.decoder(input, (hn, cn))\n",
        "\n",
        "\t\t\t\t\t\tteacher_force = random.random() < teacher_forcing_ratio   # teacher forcing draw\n",
        "\n",
        "\t\t\t\t\t\tshape = output.shape\n",
        "\n",
        "\t\t\t\t\t\t# we add one dimension to classify all bacthes at same time\n",
        "\t\t\t\t\t\tx=output.unsqueeze(2)\n",
        "\n",
        "\t\t\t\t\t\tx = self.classifier(x) #softmax is'nt necessary\n",
        "\n",
        "\t\t\t\t\t\t# removing the extra dimension\n",
        "\t\t\t\t\t\tx = x.view(shape[0],shape[1],-1)\n",
        "\n",
        "\t\t\t\t\t\t# applying the threshold to the predicted value\n",
        "\t\t\t\t\t\toutput = (x > self.thr).float()\n",
        "\n",
        "\t\t\t\t\t\t# we choose the predicted or real note for the next iteration based on teacher forcing random draw\n",
        "\t\t\t\t\t\tinput = (y[:,t,:].view(y.shape[0],1,y.shape[2]) if teacher_force else output.view(y.shape[0],1,y.shape[2]))\n",
        "\n",
        "\t\t\t\t\t\t# we accumulate the new note\n",
        "\t\t\t\t\t\toutputs[:,t,:] = x.view(y.shape[0],-1)\n",
        "\n",
        "\t\t\t\treturn outputs\n",
        "\n",
        "\t\tdef loss(self, x,y): #Standard BCE loss\n",
        "\t\t\t\tx = x.view(-1,x.shape[2])\n",
        "\t\t\t\ty_pred = y.view(-1,y.shape[2])\n",
        "\n",
        "\t\t\t\treturn self.loss_function(x,y_pred)\n",
        "\n",
        "\t\tdef focal_loss(self, x, y, alpha = 0.5, gamma=2.0): #BCE with focal loss\n",
        "\t\t\t\tx = x.view(-1,x.shape[2])\n",
        "\t\t\t\ty = y.view(-1,y.shape[2])\n",
        "\n",
        "\t\t\t\tt = y.float()\n",
        "\n",
        "\t\t\t\tp = x.sigmoid().detach()\n",
        "\t\t\t\tpt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
        "\t\t\t\tw = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
        "\t\t\t\tw = w * (1-pt).pow(gamma)\n",
        "\t\t\t\treturn F.binary_cross_entropy_with_logits(x, t, w, reduction='sum')\n",
        "\n",
        "\t\tdef recall(self,x,y):\n",
        "\t\t\t\tx_pred = (x > self.thr).long() #if BCELoss expects sigmoid -> th 0.5, BCELossWithLogits expect real values -> th 0.0\n",
        "\t\t\t\treturn torch.mul(x_pred.float(),y).float().sum()/y.sum()\n",
        "\n",
        "\t\tdef precision(self,x,y):\n",
        "\t\t\t\tx_pred = (x > self.thr).long() #if BCELoss expects sigmoid -> th 0.5, BCELossWithLogits expect real values -> th 0.0\n",
        "\t\t\t\treturn torch.mul(x_pred.float(),y).float().sum()/x_pred.float().sum()\n",
        "\n",
        "\t\tdef training_network(self,training_generator,learning_rate=1e-4,epochs=25, teacher_forcing_val=0.5, tearcher_forcing_strat=\"fix\", focal_alpha=0.75, focal_gamma=2.0):\n",
        "\t\t\t\t#define optimizer\n",
        "\t\t\t\toptimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "\t\t\t\tloss_train=[]\n",
        "\t\t\t\trecall_train=[]\n",
        "\t\t\t\tprecision_train=[]\n",
        "\t\t\t\tdensity_train=[]\n",
        "\n",
        "\t\t\t\tfor epoch in range(epochs):\n",
        "\t\t\t\t\t\tfor i,batch in enumerate(training_generator):\n",
        "\t\t\t\t\t\t\t\t# Forward pass: Compute predicted y by passing x to the model\n",
        "\t\t\t\t\t\t\t\tx = batch[0]\n",
        "\t\t\t\t\t\t\t\ty = batch[1]\n",
        "\t\t\t\t\t\t\t\tx= x.to(device)\n",
        "\t\t\t\t\t\t\t\ty= y.to(device)\n",
        "\n",
        "\t\t\t\t\t\t\t\ty_pred = self.train()(x,y,teacher_forcing_ratio=teacher_forcing_val*(1-epoch/epochs)**2)\n",
        "\n",
        "\t\t\t\t\t\t\t\t# Compute and print loss\n",
        "\t\t\t\t\t\t\t\tloss = self.focal_loss(y_pred, y, alpha=focal_alpha, gamma=focal_gamma)\n",
        "\t\t\t\t\t\t\t\trecall  = self.recall(y_pred, y)\n",
        "\t\t\t\t\t\t\t\tprecision = self.precision(y_pred, y)\n",
        "\n",
        "\t\t\t\t\t\t\t\t# Zero gradients, perform a backward pass, and update the weights.\n",
        "\t\t\t\t\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\t\t\t\t\tloss.backward()\n",
        "\t\t\t\t\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\t\t\tloss_train.append(loss.item())\n",
        "\n",
        "\t\t\t\t\t\t#how many of the notes on the composition where predicted\n",
        "\t\t\t\t\t\trecall_train.append(recall.item())\n",
        "\n",
        "\t\t\t\t\t\t#how many of the notes predicted followed the composition\n",
        "\t\t\t\t\t\tprecision_train.append(precision.item())\n",
        "\n",
        "\t\t\t\t\t\t#represents the density of the pianoroll. Good values for 176 (COD_TYPE=2) 4 voices tend to be arround 0.025\n",
        "\t\t\t\t\t\tdensity_train.append((y_pred>self.thr).float().mean().item())\n",
        "\n",
        "\t\t\t\t\t\tif epoch%5==0:\n",
        "\t\t\t\t\t\t\t\ttorch.save(self, 'models/model_partial_{0}.pt'.format(epoch))\n",
        "\n",
        "\t\t\t\t\t\tprint(\"Epoch: {}, Loss: {:06.2f}, Recall: {:06.4f}, Precision: {:06.4f}, Density: {:06.4f}\".format(epoch, loss_train[epoch], recall_train[epoch], precision_train[epoch], density_train[epoch]))\n",
        "\n",
        "\t\tdef zero_init_hidden_predict(self):\n",
        "\t\t\t\t# initialize the hidden state and the cell state to zeros\n",
        "\t\t\t\t# batch size is 1\n",
        "\t\t\t\treturn (torch.zeros(self.rnn_layers,1, self.rnn_dim),torch.zeros(self.rnn_layers,1, self.rnn_dim))\n",
        "\n",
        "\t\tdef random_init_hidden_predict(self):\n",
        "\t\t\t\t# initialize the hidden state and the cell state with a normal distribution\n",
        "\t\t\t\t# batch size is 1\n",
        "\t\t\t\treturn (torch.randn(self.rnn_layers,1, self.rnn_dim),torch.randn(self.rnn_layers,1, self.rnn_dim))\n",
        "\n",
        "\t\tdef predict(self,seq_len=500,hidden_init=\"zeros\",x_init=torch.zeros(1,1,176).to(device)):\n",
        "\n",
        "\t\t\t\t\tassert hidden_init==\"zeros\" or hidden_init==\"random\" or hidden_init==\"guided\", \"hidden_init can only take values 'zeros', 'random','guided' (in which case you have to provide x_init)\"\n",
        "\n",
        "\t\t\t\t\tself.eval()\n",
        "\n",
        "\t\t\t\t\tseq = torch.rand(1,seq_len+1,self.input_dim).to(device)\n",
        "\n",
        "\t\t\t\t\tif hidden_init==\"zeros\":\n",
        "\t\t\t\t\t\t\thn,cn=self.zero_init_hidden_predict()\n",
        "\t\t\t\t\telif hidden_init==\"random\":\n",
        "\t\t\t\t\t\t\thn,cn=self.random_init_hidden_predict()\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\toutput, (hn,cn)=self.encoder(x_init)\n",
        "\t\t\t\t\thn=hn.to(device)\n",
        "\t\t\t\t\tcn=cn.to(device)\n",
        "\n",
        "\t\t\t\t\t# for the sequence length\n",
        "\t\t\t\t\tfor t in range(seq_len):\n",
        "\n",
        "\t\t\t\t\t\t\toutput, (hn, cn) = self.decoder(seq[0,t].view(1,1,-1),(hn,cn))\n",
        "\n",
        "\t\t\t\t\t\t\tshape = output.shape\n",
        "\n",
        "\t\t\t\t\t\t\tx=output.unsqueeze(2)\n",
        "\n",
        "\t\t\t\t\t\t\tx = self.classifier(x) #no hace falta la softmax\n",
        "\n",
        "\t\t\t\t\t\t\tx = x.view(shape[0],shape[1],-1)\n",
        "\n",
        "\t\t\t\t\t\t\toutput = (x > self.thr).float()\n",
        "\n",
        "\t\t\t\t\t\t\tseq[:,t,:] = output.view(shape[0],-1)\n",
        "\n",
        "\t\t\t\t\tseq = seq[0][1:][:].cpu().detach().numpy()\n",
        "\t\t\t\t\treturn seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3fc1880e-b635-463e-ad60-d69cbdfa3db1",
      "metadata": {
        "id": "3fc1880e-b635-463e-ad60-d69cbdfa3db1"
      },
      "outputs": [],
      "source": [
        "QNSF=4\n",
        "COD_TYPE=2\n",
        "SEQ_LEN=128\n",
        "BATCH_SIZE=16\n",
        "EPOCHS=20\n",
        "DATA_LEN_TRAIN=10\n",
        "DATA_LEN_VAL=5\n",
        "DATA_LEN_TEST=5\n",
        "RNN_DIM=512\n",
        "RNN_LAYERS=2\n",
        "TEACHER_FORCING=0.5\n",
        "ALPHA=0.65\n",
        "GAMMA=2.0\n",
        "LR=1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe018d01-09f5-4cc1-ad82-121ea6234fe0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe018d01-09f5-4cc1-ad82-121ea6234fe0",
        "outputId": "8003d746-899a-4159-9d88-4539b41d3550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00573_Cover_Arrangement_B.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00572_Cover_Timbre_D.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00573_Cover_Genre_A.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00572_Cover_Instrument_D.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00571_Cover_Instrument_D.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00138_Cover_Timbre_A.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00572_Cover_Instrument_B.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00610_Cover_Genre_A.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00572_Cover_Tempo_A.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00571_Cover_Timbre_D.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00571_Cover_Rhythm_C.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00585_Cover_Rhythm_C.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00571_Cover_Tempo_A.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00571_Cover_Tempo_C.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00572_Cover_Rhythm_C.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00572_Cover_Arrangement_B.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00608_Cover_Instrument_D.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00138_Cover_Timbre_C.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00572_Cover_Genre_A.mid\n",
            "Processing file /content/drive/MyDrive/midi/Similar_Trot_00571_Cover_Timbre_B.mid\n"
          ]
        }
      ],
      "source": [
        "#DataLoader and DataGenerator\n",
        "MIDI_SOURCE = '/content/drive/MyDrive/midi'\n",
        "#Read Files (returns a 3-d tuple of list of files)\n",
        "midi_files = list_midi_files(MIDI_SOURCE, DATA_LEN_TRAIN, DATA_LEN_VAL, DATA_LEN_TEST, randomSeq=True)\n",
        "\n",
        "#Init Midi Model\n",
        "dataset_train=MidiDataset(qnsf=QNSF, seq_len=SEQ_LEN, cod_type=COD_TYPE, midi_files=midi_files[0])\n",
        "dataset_val=MidiDataset(qnsf=QNSF, seq_len=SEQ_LEN, cod_type=COD_TYPE, midi_files=midi_files[1])\n",
        "dataset_test=MidiDataset(qnsf=QNSF, seq_len=SEQ_LEN, cod_type=COD_TYPE, midi_files=midi_files[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "207dd987-e90a-4bfb-b988-e10f3b3bebf0",
      "metadata": {
        "id": "207dd987-e90a-4bfb-b988-e10f3b3bebf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "1f43c49c-ae3c-4784-da36-6b8d68bb4c88"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-eab7e7bc84fb>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Generate Data Loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtraining_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mvalidating_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE_VAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtesting_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE_TEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset_train' is not defined"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE_TRAIN=BATCH_SIZE  # batch for the train\n",
        "BATCH_SIZE_VAL=BATCH_SIZE\n",
        "BATCH_SIZE_TEST=BATCH_SIZE\n",
        "\n",
        "#Generate Data Loader\n",
        "training_generator = data.DataLoader(dataset_train, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
        "validating_generator = data.DataLoader(dataset_val, batch_size=BATCH_SIZE_VAL, shuffle=True)\n",
        "testing_generator = data.DataLoader(dataset_test, batch_size=BATCH_SIZE_TEST, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "84545802-1df5-4f48-ab54-892c96d04a8c",
      "metadata": {
        "id": "84545802-1df5-4f48-ab54-892c96d04a8c"
      },
      "outputs": [],
      "source": [
        "#Init Model\n",
        "\n",
        "model = Seq2Seq(input_dim=88*COD_TYPE, rnn_dim=RNN_DIM, rnn_layers=RNN_LAYERS, thr=0)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f8722293-1c79-4c69-9fd8-dcf7ed8e480b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "f8722293-1c79-4c69-9fd8-dcf7ed8e480b",
        "outputId": "b1d30737-04fa-4d34-c210-f78b2358d3ee"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'training_generator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bf423b568ab3>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEACHER_FORCING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtearcher_forcing_strat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocal_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mALPHA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocal_gamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_generator' is not defined"
          ]
        }
      ],
      "source": [
        "#Train Model\n",
        "import os\n",
        "# Define the directory path where models will be saved\n",
        "models_dir = 'models'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "\n",
        "model.training_network(training_generator,learning_rate=LR,epochs=EPOCHS, teacher_forcing_val=TEACHER_FORCING, tearcher_forcing_strat=\"fix\", focal_alpha=ALPHA, focal_gamma=GAMMA)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cdac389-c68b-4466-a6a8-3a66b814486a",
      "metadata": {
        "id": "2cdac389-c68b-4466-a6a8-3a66b814486a"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'models/model_def_{0}_{1}songs.pt'.format(EPOCHS,DATA_LEN_TRAIN))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils import data\n",
        "import torch\n",
        "\n",
        "def generating(MODEL_PATH,MIDI_FILE,SEQ_LEN=200,HIDDEN_INIT=\"zeros\"):\n",
        "\n",
        "\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\tseq2seqMod = torch.load(MODEL_PATH, map_location=device)\n",
        "\n",
        "\tsequence=seq2seqMod.to(device).predict(seq_len=SEQ_LEN,hidden_init=HIDDEN_INIT)\n",
        "\n",
        "\tcreate_midi(sequence, qnsf = 4, cod_type=2, midiOutputFile=MIDI_FILE)\n",
        "\n",
        "\treturn sequence\n",
        "\n",
        "\n",
        "generating(MODEL_PATH='/content/model_def_20_10songs.pt',SEQ_LEN=500,HIDDEN_INIT='random',MIDI_FILE='/content/output.mid')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l_OM6sxxaxj",
        "outputId": "1f6e1d91-3c37-4762-f875-3b0079ea5ee7"
      },
      "id": "_l_OM6sxxaxj",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created midi file:  /content/output.mid\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.84891284, 0.05509884, 0.12613666, ..., 0.3560839 , 0.14366502,\n",
              "        0.8119226 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.load('/content/model_def_20_10songs.pt', map_location=device)\n",
        "\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWlYQ476yd7E",
        "outputId": "cf425b13-e930-4518-e3a4-72a153b5558e"
      },
      "id": "SWlYQ476yd7E",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): LSTM(176, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (decoder): LSTM(176, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=176, bias=True)\n",
            "  )\n",
            "  (loss_function): BCEWithLogitsLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model parameters\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter: {name}, Shape: {param.shape}\")\n",
        "\n",
        "# Check total number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htlJW05pzUxz",
        "outputId": "2abcd8a5-0040-4ca2-ce8c-f580c9b07f85"
      },
      "id": "htlJW05pzUxz",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter: encoder.weight_ih_l0, Shape: torch.Size([2048, 176])\n",
            "Parameter: encoder.weight_hh_l0, Shape: torch.Size([2048, 512])\n",
            "Parameter: encoder.bias_ih_l0, Shape: torch.Size([2048])\n",
            "Parameter: encoder.bias_hh_l0, Shape: torch.Size([2048])\n",
            "Parameter: encoder.weight_ih_l1, Shape: torch.Size([2048, 512])\n",
            "Parameter: encoder.weight_hh_l1, Shape: torch.Size([2048, 512])\n",
            "Parameter: encoder.bias_ih_l1, Shape: torch.Size([2048])\n",
            "Parameter: encoder.bias_hh_l1, Shape: torch.Size([2048])\n",
            "Parameter: decoder.weight_ih_l0, Shape: torch.Size([2048, 176])\n",
            "Parameter: decoder.weight_hh_l0, Shape: torch.Size([2048, 512])\n",
            "Parameter: decoder.bias_ih_l0, Shape: torch.Size([2048])\n",
            "Parameter: decoder.bias_hh_l0, Shape: torch.Size([2048])\n",
            "Parameter: decoder.weight_ih_l1, Shape: torch.Size([2048, 512])\n",
            "Parameter: decoder.weight_hh_l1, Shape: torch.Size([2048, 512])\n",
            "Parameter: decoder.bias_ih_l1, Shape: torch.Size([2048])\n",
            "Parameter: decoder.bias_hh_l1, Shape: torch.Size([2048])\n",
            "Parameter: classifier.0.weight, Shape: torch.Size([256, 512])\n",
            "Parameter: classifier.0.bias, Shape: torch.Size([256])\n",
            "Parameter: classifier.3.weight, Shape: torch.Size([176, 256])\n",
            "Parameter: classifier.3.bias, Shape: torch.Size([176])\n",
            "Total number of parameters: 7205296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect encoder LSTM weights\n",
        "print(model.encoder.weight_ih_l0)\n",
        "print(model.encoder.weight_hh_l0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waGT_3j2zdto",
        "outputId": "43448b73-f4fd-493f-d9fa-e33055d695a3"
      },
      "id": "waGT_3j2zdto",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0067,  0.0426,  0.0078,  ...,  0.0282,  0.0234,  0.0057],\n",
            "        [ 0.0417, -0.0284,  0.0216,  ...,  0.0132,  0.0025,  0.0377],\n",
            "        [-0.0437, -0.0269, -0.0379,  ..., -0.0279,  0.0163, -0.0096],\n",
            "        ...,\n",
            "        [ 0.0149, -0.0027,  0.0034,  ...,  0.0002,  0.0091,  0.0217],\n",
            "        [-0.0070,  0.0157, -0.0289,  ..., -0.0391,  0.0221, -0.0044],\n",
            "        [ 0.0214, -0.0301, -0.0361,  ...,  0.0362,  0.0392,  0.0112]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0557, -0.0408, -0.0067,  ..., -0.0248, -0.0149, -0.0476],\n",
            "        [ 0.0341, -0.0020, -0.0044,  ...,  0.0595, -0.0063,  0.0354],\n",
            "        [-0.0159, -0.0566,  0.0252,  ...,  0.0271, -0.0408,  0.0065],\n",
            "        ...,\n",
            "        [-0.0023, -0.0166,  0.0085,  ..., -0.0004, -0.0369,  0.0081],\n",
            "        [-0.0229,  0.0308,  0.0076,  ...,  0.0234, -0.0503, -0.0139],\n",
            "        [-0.0276, -0.0035,  0.0025,  ...,  0.0198, -0.0043,  0.0034]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming predict method is implemented correctly\n",
        "sequence = model.predict(seq_len=100, hidden_init='random')\n",
        "print(sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "6ksjWVTSzoSp",
        "outputId": "7e6a57c0-123f-429b-86c8-326f804357e0"
      },
      "id": "6ksjWVTSzoSp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f4dc7b072278>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming predict method is implemented correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
